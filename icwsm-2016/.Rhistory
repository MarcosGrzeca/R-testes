options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
DATABASE <- "icwsm-2016"
dados <- query("SELECT id, q1, q2, q3, textParset, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
dados$repetitions[dados$emoticonPos > 0] <- 1
dados$repetitions[dados$emoticonPos == 0] <- 0
dados$emoticonPos[dados$emoticonPos > 0] <- 1
dados$emoticonPos[dados$emoticonPos == 0] <- 0
dados$emoticonNeg[dados$emoticonNeg > 0] <- 1
dados$emoticonNeg[dados$emoticonNeg == 0] <- 0
dados$q1 <- as.factor(dados$q1)
dados$q2 <- as.factor(dados$q2)
dados$q3 <- as.factor(dados$q3)
library(text2vec)
library(data.table)
setkey(dados, id)
prep_fun = tolower
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$texto,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
doc_proportion_min = 0.25)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer = vocab_vectorizer(vocab)
vectorizer = vocab_vectorizer(pruned_vocab)
dtm_train = create_dtm(it_train, vectorizer)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
doc_proportion_min = 0.25)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
pruned_vocab = prune_vocabulary(vocab)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
dtm_train
str(dtm_train)
pruned_vocab = prune_vocabulary(vocab)
vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, vectorizer)
str(dtm_train)
ncol(dtm_train)
nrow(dtm_train)
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
doc_proportion_min = 0.25)
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
ncol(dtm_train)
nrow(dtm_train)
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
doc_proportion_min = 0.99)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
ncol(dtm_train)
nrow(dtm_train)
str(dtm_train)
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
#doc_proportion_min = 0.25
max_number_of_terms = 20)
vectorizer = vocab_vectorizer(vocab)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
ncol(dtm_train)
nrow(dtm_train)
str(dtm_train)
viw(dtm_train)
view(dtm_train)
dtm_train
dataM <- as.data.frame(as.matrix(dtm_train))
view(dataM)
View(dataM)
get_tf(dtm_train, norm = c("l1", "l2"))
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
#doc_proportion_min = 0.25
max_number_of_terms = 200)
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
get_tf(dtm_train, norm = c("l1", "l2"))
ncol(dtm_train)
nrow(dtm_train)
dtm_train
dataM <- as.data.frame(as.matrix(dtm_train))
View(dataM)
vocab
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S' LIMIT 10")
dados$q1 <- as.factor(dados$q1)
dados$q2 <- as.factor(dados$q2)
dados$q3 <- as.factor(dados$q3)
dados$emoticonPos[dados$emoticonPos > 0] <- 1
dados$emoticonPos[dados$emoticonPos == 0] <- 0
dados$emoticonNeg[dados$emoticonNeg > 0] <- 1
dados$emoticonNeg[dados$emoticonNeg == 0] <- 0
clearConsole()
library(text2vec)
library(data.table)
setDT(dados)
setkey(dados, id)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
vocab
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
#doc_proportion_min = 0.25
max_number_of_terms = 200)
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
ncol(dtm_train)
nrow(dtm_train)
dtm_train
dataM <- as.data.frame(as.matrix(dtm_train))
View(dataM)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
DATABASE <- "icwsm-2016"
clearConsole();
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
dados$q1 <- as.factor(dados$q1)
dados$q2 <- as.factor(dados$q2)
dados$emoticonPos[dados$emoticonPos == 0] <- 0
dados$emoticonPos[dados$emoticonPos > 0] <- 1
dados$emoticonNeg[dados$emoticonNeg > 0] <- 1
clearConsole()
dados$emoticonNeg[dados$emoticonNeg == 0] <- 0
dados$q3 <- as.factor(dados$q3)
library(text2vec)
library(data.table)
setDT(dados)
setkey(dados, id)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
library(qdap)
install.packages("qdap")
if (!require("qdap")) {
install.packages("qdap")
}
if (!require("qdap")) {
install.packages("qdap")
}
library(qdap)
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
frequent_terms <- freq_terms(vocab, 25)
frequent_terms
vectorizer = vocab_vectorizer(frequent_terms)
vocab
library(qdap)
frequent_terms <- freq_terms(vocab, 25)
frequent_terms
vocab
frequent_terms
frequent_terms <- freq_terms(vocab, 20)
frequent_terms <- freq_terms(vocab, 10)
vocab
frequent_terms
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
vocab
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
frequent_terms <- freq_terms(dados$textParser, 3)
frequent_terms
frequent_terms <- freq_terms(dados$textParser,20)
it_train = itoken(frequent_terms,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
frequent_terms <- freq_terms(dados$textParser,20)
frequent_terms
