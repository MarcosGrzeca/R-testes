ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
doc_proportion_min = 0.25)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer = vocab_vectorizer(vocab)
vectorizer = vocab_vectorizer(pruned_vocab)
dtm_train = create_dtm(it_train, vectorizer)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
doc_proportion_min = 0.25)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
pruned_vocab = prune_vocabulary(vocab)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
dtm_train
str(dtm_train)
pruned_vocab = prune_vocabulary(vocab)
vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, vectorizer)
str(dtm_train)
ncol(dtm_train)
nrow(dtm_train)
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
doc_proportion_min = 0.25)
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
ncol(dtm_train)
nrow(dtm_train)
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
doc_proportion_min = 0.99)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
ncol(dtm_train)
nrow(dtm_train)
str(dtm_train)
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
#doc_proportion_min = 0.25
max_number_of_terms = 20)
vectorizer = vocab_vectorizer(vocab)
vectorizer = vocab_vectorizer(pruned_vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
ncol(dtm_train)
nrow(dtm_train)
str(dtm_train)
viw(dtm_train)
view(dtm_train)
dtm_train
dataM <- as.data.frame(as.matrix(dtm_train))
view(dataM)
View(dataM)
get_tf(dtm_train, norm = c("l1", "l2"))
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
#doc_proportion_min = 0.25
max_number_of_terms = 200)
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
get_tf(dtm_train, norm = c("l1", "l2"))
ncol(dtm_train)
nrow(dtm_train)
dtm_train
dataM <- as.data.frame(as.matrix(dtm_train))
View(dataM)
vocab
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S' LIMIT 10")
dados$q1 <- as.factor(dados$q1)
dados$q2 <- as.factor(dados$q2)
dados$q3 <- as.factor(dados$q3)
dados$emoticonPos[dados$emoticonPos > 0] <- 1
dados$emoticonPos[dados$emoticonPos == 0] <- 0
dados$emoticonNeg[dados$emoticonNeg > 0] <- 1
dados$emoticonNeg[dados$emoticonNeg == 0] <- 0
clearConsole()
library(text2vec)
library(data.table)
setDT(dados)
setkey(dados, id)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
vocab
pruned_vocab = prune_vocabulary(vocab,
#term_count_min = 10,
#doc_proportion_max = 0.5,
#doc_proportion_min = 0.25
max_number_of_terms = 200)
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
ncol(dtm_train)
nrow(dtm_train)
dtm_train
dataM <- as.data.frame(as.matrix(dtm_train))
View(dataM)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
DATABASE <- "icwsm-2016"
clearConsole();
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
dados$q1 <- as.factor(dados$q1)
dados$q2 <- as.factor(dados$q2)
dados$emoticonPos[dados$emoticonPos == 0] <- 0
dados$emoticonPos[dados$emoticonPos > 0] <- 1
dados$emoticonNeg[dados$emoticonNeg > 0] <- 1
clearConsole()
dados$emoticonNeg[dados$emoticonNeg == 0] <- 0
dados$q3 <- as.factor(dados$q3)
library(text2vec)
library(data.table)
setDT(dados)
setkey(dados, id)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
library(qdap)
install.packages("qdap")
if (!require("qdap")) {
install.packages("qdap")
}
if (!require("qdap")) {
install.packages("qdap")
}
library(qdap)
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
frequent_terms <- freq_terms(vocab, 25)
frequent_terms
vectorizer = vocab_vectorizer(frequent_terms)
vocab
library(qdap)
frequent_terms <- freq_terms(vocab, 25)
frequent_terms
vocab
frequent_terms
frequent_terms <- freq_terms(vocab, 20)
frequent_terms <- freq_terms(vocab, 10)
vocab
frequent_terms
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
vocab
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
frequent_terms <- freq_terms(dados$textParser, 3)
frequent_terms
frequent_terms <- freq_terms(dados$textParser,20)
it_train = itoken(frequent_terms,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
frequent_terms <- freq_terms(dados$textParser,20)
frequent_terms
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
DATABASE <- "icwsm-2016"
clearConsole();
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
dados$q1 <- as.factor(dados$q1)
dados$q2 <- as.factor(dados$q2)
dados$q3 <- as.factor(dados$q3)
dados$emoticonPos[dados$emoticonPos > 0] <- 1
dados$emoticonPos[dados$emoticonPos == 0] <- 0
dados$emoticonNeg[dados$emoticonNeg > 0] <- 1
dados$emoticonNeg[dados$emoticonNeg == 0] <- 0
clearConsole()
library(text2vec)
library(data.table)
setDT(dados)
setkey(dados, id)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
if (!require("qdap")) {
install.packages("qdap")
}
library(qdap)
frequent_terms <- freq_terms(dados$textParser,20)
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
frequent_terms <- freq_terms(dados$textParser,20)
frequent_terms
stopwords = tm::stopwords("en")
stopwords = tm::stopwords("de")
stopwords
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
DATABASE <- "icwsm-2016"
clearConsole();
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
dados$q1 <- as.factor(dados$q1)
dados$q2 <- as.factor(dados$q2)
dados$q3 <- as.factor(dados$q3)
dados$emoticonPos[dados$emoticonPos > 0] <- 1
dados$emoticonPos[dados$emoticonPos == 0] <- 0
dados$emoticonNeg[dados$emoticonNeg > 0] <- 1
dados$emoticonNeg[dados$emoticonNeg == 0] <- 0
clearConsole()
carregar("text2vec");
install.packages(pacote)
carregar(text2vec);
carregar(text2vec);
instalar("text2vec")
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
DATABASE <- "icwsm-2016"
clearConsole();
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
dados$q1 <- as.factor(dados$q1)
dados$q2 <- as.factor(dados$q2)
dados$q3 <- as.factor(dados$q3)
dados$emoticonPos[dados$emoticonPos > 0] <- 1
dados$emoticonPos[dados$emoticonPos == 0] <- 0
dados$emoticonNeg[dados$emoticonNeg > 0] <- 1
dados$emoticonNeg[dados$emoticonNeg == 0] <- 0
clearConsole()
instalar("text2vec")
install.packages(pacote)
instalar("text2vec")
instalar("text2vec")
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
setDT(dados)
setkey(dados, id)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
if (!require("qdap")) {
install.packages("qdap")
}
library(qdap)
frequent_terms <- freq_terms(dados$textParser,20)
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
topfeatures(dados$textParser, n = 10, decreasing = TRUE, ci = 0.95)
if (!require("quanteda")) {
install.packages("quanteda")
}
library(quanteda)
topfeatures(dados$textParser, n = 10, decreasing = TRUE, ci = 0.95)
if (!require("qdap")) {
install.packages("qdap")
}
library(qdap)
frequent_terms <- freq_terms(dados$textParser,20)
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
topfeatures(dfm(corpus_subset(dados$textParser), verbose = FALSE))
frequent_terms <- freq_terms(dados$textParser,20)
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
vocab
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
vocab
frequent_terms
vocab
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
vocab = create_vocabulary(it_train, ngram = c(3L, 3L), sep_ngram = "+")
vocab
library(qdap)
frequent_terms <- freq_terms(dados$textParser,20)
vocab
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
vocab
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
frequent_terms <- freq_terms(vocab, 10)
frequent_terms
vocab = create_vocabulary(it_train, ngram = c(3L, 3L), sep_ngram = "AA")
vocab
vocab
frequent_terms <- freq_terms(vocab, 10)
frequent_terms
frequent_terms <- freq_terms(vocab, 5)
frequent_terms
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
vocab = create_vocabulary(it_train, ngram = c(3L, 3L), sep_ngram = "____")
vocab
frequent_terms <- freq_terms(vocab, 3)
frequent_terms
vocab
options(max.print = 99999999)
library(tools)
source(file_path_as_absolute("functions.R"))
DATABASE <- "icwsm-2016"
clearConsole();
dados <- query("SELECT id, q1, q2, q3, textParser, hashtags, emoticonPos, emoticonNeg FROM tweets WHERE situacao = 'S'")
dados$q1 <- as.factor(dados$q1)
dados$q2 <- as.factor(dados$q2)
dados$q3 <- as.factor(dados$q3)
dados$emoticonPos[dados$emoticonPos > 0] <- 1
dados$emoticonPos[dados$emoticonPos == 0] <- 0
dados$emoticonNeg[dados$emoticonNeg > 0] <- 1
dados$emoticonNeg[dados$emoticonNeg == 0] <- 0
clearConsole()
if (!require("text2vec")) {
install.packages("text2vec")
}
library(text2vec)
library(data.table)
setDT(dados)
setkey(dados, id)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(dados$textParser,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, ngram = c(3L, 3L), sep_ngram = "____")
vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
vocab
vocab
vectorizer = vocab_vectorizer(vocab)
vectorizer
dtm_train = create_dtm(it_train, vectorizer)
ncol(dtm_train)
nrow(dtm_train)
dtm_train
dataM <- as.data.frame(as.matrix(dtm_train))
view(dataM)
View(dataM)
data <- as.data.table(as.matrix(dtm_train))
data <- as.matrix(dtm_train)
view(data)
View(data)
View(dataM)
cols <- colnames(data)
cols
colSums(data[,-1])
colSums(data$ec_ga_mention[,-1])
colSums(data$ec_ga_mention)
colSums(data["ec_ga_mention",-1])
colSums(data["ec_ga_mention"])
colSums(data["ec_ga_mention",1])
data["ec_ga_mention",-1]
data[1, "ec_ga_mention"]
data[-1, "ec_ga_mention"]
colSums(data[-1, "ec_ga_mention"])
colSums(data[,-1])
colSums(data[-1, "ec_ga_mention"])
data[-1, "ec_ga_mention"]
colSums(data[-1, "ec_ga_mention"])
dataFrame <- as.data.frame(as.matrix(dtm_train))
dataFram[-1, "ec_ga_mention"]
dataFrame[-1, "ec_ga_mention"]
colSums(dataFrame[-1, "ec_ga_mention"])
colSums(dataFrame$ec_ga_mention)
dataFrame$ec_ga_mention
colSums(dataFrame[-1, "ec_ga_mention"])
rowSums(dataFrame[-1, "ec_ga_mention"])
rowsum(dataFrame[-1, "ec_ga_mention"])
rowsum(dataFrame["ec_ga_mention"])
colSum(data[-1, "ec_ga_mention"])
colSums(data[-1, "ec_ga_mention"])
colSums(data)
ret
ret <- colSums(data)
ret
ret[1]
sort(ret)
sort(ret, decreasing = TRUE)
aspectos <- sort(colSums(data), decreasing = TRUE)
aspectos
nrow(aspectos)
counts(aspectos)
length(aspectos)
manter <- length(aspectos) * 0.25
manter
manter <- round(length(aspectos) * 0.25)
manter
for(i in 1:cols) {
#dadosFinal[[cols[i]]] <- as.integer(dadosFinal[[cols[i]]])
#colSums(data[,-1])
print(i)
}
print(cols[i])
for(i in 1:ncol(cols)) {
#dadosFinal[[cols[i]]] <- as.integer(dadosFinal[[cols[i]]])
#colSums(data[,-1])
print(cols[i])
}
for(i in 1:nrow(cols)) {
#dadosFinal[[cols[i]]] <- as.integer(dadosFinal[[cols[i]]])
#colSums(data[,-1])
print(cols[i])
}
cols
nrow(cols)
length(cols)
for(i in 1:length(cols)) {
#dadosFinal[[cols[i]]] <- as.integer(dadosFinal[[cols[i]]])
#colSums(data[,-1])
print(cols[i])
}
for(i in 1:length(aspectos)) {
print(aspectos[i])
}
manter <- c()
remover <- c()
for(i in 1:length(aspectos)) {
#print(aspectos[i])
if (i <= manter) {
aspectosManter <- c(aspectosManter, aspectos[i])
}
}
aspectos <- sort(colSums(data), decreasing = TRUE)
manter <- round(length(aspectos) * 0.25)
for(i in 1:length(aspectos)) {
#print(aspectos[i])
if (i <= manter) {
aspectosManter <- c(aspectosManter, aspectos[i])
}
}
aspectosManter <- c()
aspectosRemover <- c()
for(i in 1:length(aspectos)) {
#print(aspectos[i])
if (i <= manter) {
aspectosManter <- c(aspectosManter, aspectos[i])
}
}
aspectosManter
for(i in 1:length(aspectos)) {
#print(aspectos[i])
#if (i <= manter) {
if (i <= 10) {
aspectosManter <- c(aspectosManter, aspectos[i])
}
}
aspectosManter
aspectos <- sort(colSums(data), decreasing = TRUE)
manter <- round(length(aspectos) * 0.25)
aspectosManter <- c()
aspectosRemover <- c()
for(i in 1:length(aspectos)) {
#print(aspectos[i])
#if (i <= manter) {
if (i <= 10) {
aspectosManter <- c(aspectosManter, aspectos[i])
}
}
aspectosManter
aspectos
aspectos[1:10]
